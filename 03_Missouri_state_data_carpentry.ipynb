{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Carpentry on the Aggrigated Zip Code Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal is to make a single dataframe per category containing all ten years worth of data for that category. The categories are as follows.\n",
    "\n",
    "* Home Owners - loss;       homelos\n",
    "* Home Owners - exposure;   homeexp\n",
    "* Farm Owners - loss;       farmlos\n",
    "* Farm Owners - exposure;   farmexp\n",
    "* Mobile Home - loss;       mobilelos\n",
    "* Mobile Home - exposure;   mobileexp\n",
    "* Earthquake - loss;        earthlos\n",
    "* Earthquake - exposure;    earthexp\n",
    "* Automobile - loss;        autolos\n",
    "* Automobile - exposure;    autoexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sqlalchemy as sqla\n",
    "import sqlite3 as sql3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function walks through a designated folder containing data files. These files are stored in folders by year and caregory. It then loads the files into separate dataframes, and then concatenates the different years of data together into a single dataframe of all years for that category. Lastly, it loads the data into a sqlite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_and_process(directory):\n",
    "    '''\n",
    "    Function to walk through a directory and process all data files into pandas dataframes.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): The directory to walk through.\n",
    "    '''\n",
    "    \n",
    "    # Create an empty list to store the directories\n",
    "    DIRS = []\n",
    "\n",
    "    # Walk through the directory and store the directories in the DIRS list\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "\n",
    "        for file in files:\n",
    "\n",
    "            if file.startswith('.'):\n",
    "\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Split the file path and store the directories in the DIRS list\n",
    "                lst = file_path.split('/')\n",
    "\n",
    "                # Append the year, category, and filename to the DIRS list\n",
    "                DIRS.append(lst[8:])\n",
    "\n",
    "    # Create a dataframe for each file in the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "\n",
    "        for file in files:\n",
    "\n",
    "            globals()[file] = pd.read_table(os.path.join(root, file), sep='\\t')\n",
    "\n",
    "    # Create a master dataframe for each type of data\n",
    "    master_dfs = [\n",
    "        'homelos',\n",
    "        'homeexp',\n",
    "        'farmlos',\n",
    "        'farmexp',\n",
    "        'mobilelos',\n",
    "        'mobileexp',\n",
    "        'earthlos',\n",
    "        'earthexp',\n",
    "        'autolos',\n",
    "        'autoexp'    \n",
    "    ]\n",
    "\n",
    "    # Process the data in each dataframe\n",
    "    for df in master_dfs:\n",
    "\n",
    "        globals()[df] = pd.DataFrame()\n",
    "\n",
    "        for d in DIRS:\n",
    "\n",
    "            # Add the year to the dataframe.\n",
    "            globals()[d[-1]]['year'] = d[0]\n",
    "\n",
    "            for c in globals()[d[-1]].columns:\n",
    "                \n",
    "                # Check if the column is an object\n",
    "                if globals()[d[-1]][c].dtype == 'object':\n",
    "                    \n",
    "                    # Remove $ and , from the columns\n",
    "                    globals()[d[-1]][c] = globals()[d[-1]][c].str.replace('$', '')\n",
    "                    globals()[d[-1]][c] = globals()[d[-1]][c].str.replace(',', '')\n",
    "                \n",
    "                try:\n",
    "\n",
    "                    # Convert the column to numeric\n",
    "                    globals()[d[-1]][c] = pd.to_numeric(globals()[d[-1]][c], errors='coerce')\n",
    "\n",
    "                except Exception as e:\n",
    "\n",
    "                    print(e)\n",
    "                    #print(f'Could not convert column {c} to numeric in {df[-1]}')\n",
    "\n",
    "            if df in d[-1]:\n",
    "\n",
    "                # Concatenate the dataframes that are the same category\n",
    "                globals()[df] = pd.concat([globals()[df], globals()[d[-1]]], ignore_index=True)\n",
    "\n",
    "    # Drop columns that are all NaN\n",
    "    for df in master_dfs:\n",
    "        \n",
    "        globals()[df].dropna(axis=1, how='all', inplace=True)\n",
    "        globals()[df]['zip'] = globals()[df]['zip'].astype(str)\n",
    "        globals()[df]['year'] = globals()[df]['year'].astype(str)\n",
    "\n",
    "    # return the list of dataframe names\n",
    "    return master_dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the directory and run the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_to_walk = '/Users/skylar_calvin/Library/CloudStorage/OneDrive-UniversityofMissouri/Data/All Data Requested'\n",
    "\n",
    "dfs = walk_and_process(directory_to_walk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to the SQLite database created in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqla.create_engine('sqlite:///life_data.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now load the data into the life_data sqlite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    \n",
    "    globals()[df].to_sql(name = df, con = engine, if_exists = 'replace', index = False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
